---
title: "Modeling cells - quarto + EDA + tidymodels"
format: html
editor: visual
output: github_document
author: 'Tural Sadigov'
date: "`r Sys.Date()`"
---

We will be investigating 'cells' data from modeldata package in R. Lets load all the packages we will be using.

```{r}
library(modeldata)
library(tidymodels)
library(corrplot)
```

Let's glimpse at the data.

```{r}
data(cells)
glimpse(cells)
```

Most of the variables are numerical (double and integer) measuring cell properties. Case variable is the label for training and testing,, and class variable which we will be predicting has two levels.

```{r}
cells %>% 
  ggplot(aes(x = case)) +
  geom_bar()
cells %>% 
  ggplot(aes(x = class)) +
  geom_bar()
```

It seems that data is already split approximately 50-50 for training and testing, and classes are not balanced. Dimensions of the data:

```{r}
dim(cells)
```

Only 2K observations with 58 predictors. We could look at summary of some variables in the data.

```{r}
cells %>% 
  select(colnames(cells)[1:5]) %>% 
  summary()
```

It does seem that many numerical predictors have positive skew. Classes are distributed by approximately 65-35%.

```{r}
cells %>% 
  select(where(is.numeric)) %>% 
  boxplot()
```

Some predictors have many outliers and many predictors are not on the same scale. Let's look at the correlations between numerical predictors.

```{r}
cells %>% 
  select(where(is.numeric)) %>%
  cor() %>% 
  corrplot::corrplot(method = 'circle', type = 'upper',
           bg="lightblue", tl.pos='n')
```

This shows many pairs of predictors are highly correlated. We could drop some and obtain maximum number of low correlated predictors using one of tidymodels (recipe) step functions or apply PCA to extract features that are not correlated at all. PCA can be done using step_pca of recipe package.

First, remove case variable (tags for train and test).

```{r}
cells <- 
  cells %>% 
  select(-case)
```

Choose 10-fold cross validation for hyper-parameter tuning.

```{r}
set.seed(123)
cell_folds <- vfold_cv(data = cells, 
                       v = 10, 
                       repeats = 1)
```

We will be doing feature engineering first.

```{r}
# model specifications with hyperparameters to be tuned
mlp_spec <- 
  mlp(hidden_units = tune(), 
      penalty = tune(), 
      epochs = tune()) %>% 
  set_engine("nnet", 
             trace = 0) %>% 
  set_mode("classification")

# recipe - make num predictors symmetric, normialize, applu pca
# with number of componnets to be tuned
# then normalize again
mlp_rec <-
  recipe(class ~ ., data = cells) %>%
  step_YeoJohnson(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_pca(all_numeric_predictors(), 
           num_comp = tune()) %>% 
  step_normalize(all_numeric_predictors())

# add both model and recipe to the workflow
mlp_wflow <- 
  workflow() %>% 
  add_model(mlp_spec) %>% 
  add_recipe(mlp_rec)
```

Extract the range for the hyper-parameters to be tuned in the model object. Note that in the model specifications, we do have three hyper-parameters: number of hidden units/neurons in the single hidden layer, the penalty for the weight decay and number of epochs.

```{r}
mlp_param <- 
  extract_parameter_set_dials(mlp_spec)
mlp_param %>% 
  extract_parameter_dials("hidden_units")
mlp_param %>% 
  extract_parameter_dials("penalty")
mlp_param %>% 
  extract_parameter_dials("epochs")
```

There is an additional hyper-parameter, number of principal components in the recipe object.

```{r}
mlp_pca_param <- 
  extract_parameter_set_dials(mlp_rec)
mlp_pca_param %>% 
  extract_parameter_dials('num_comp')
```

Lets create **`a regular grid`** for three parameters of the model/model specification above.

```{r}
tidyr::crossing(hidden_units = 1:3,
                penalty = c(0.0, 0.1),
                epochs = c(100, 200))
```

We can update our parameters' ranges by first combining all four parameter with the workflow.

```{r}
mlp_param <- 
  mlp_wflow %>% 
  extract_parameter_set_dials() %>% 
  update(
    epochs = epochs(c(50, 200)),
    num_comp = num_comp(c(0, 40))
  )
```

Evaluate the grid using cross validation.

```{r}
doParallel::registerDoParallel()
roc_res <- metric_set(roc_auc)
set.seed(145)
mlp_reg_tune <-
  mlp_wflow %>%
  tune_grid(cell_folds,
            grid = mlp_param %>% grid_regular(levels = 3),
            metrics = roc_res)
mlp_reg_tune
```

Plot the results.

```{r}
autoplot(mlp_reg_tune)+
  theme(legend.position = "top") +
  ylab('ROC-AUC') +
  xlab ('The number of Hidden Neurons')
```

Show best results!

```{r}
mlp_reg_tune %>% 
  show_best() %>% 
  select(-.estimator)

```
